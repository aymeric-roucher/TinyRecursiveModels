name: recursive_reasoning.trm@TinyRecursiveModel
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy  # More stable than stablemax

halt_exploration_prob: 0.1
halt_max_steps: 8

# Recursion cycles
y_cycles: 2  # Deep recursion cycles
z_cycles: 3  # Latent recursion steps per cycle

H_layers: 0  # Deprecated (kept for backward compatibility)
num_layers: 12  # Scaled up for ~50M parameter model

# Hidden size matches embedding dimension (384) - standard transformer practice
hidden_size: 384
num_heads: 6  # 384 / 64 = 6 heads (64-dim per head)
expansion: 4

# Pretrained embeddings from sentence-transformers
pretrained_embeddings_model: "TaylorAI/gte-tiny"
freeze_embeddings: false  # Allow fine-tuning

# No puzzle embeddings for text tasks
puzzle_emb_ndim: 0

# Causal attention for next-token prediction
causal: true

pos_encodings: rope
forward_dtype: bfloat16  # MPS supports bfloat16 well

mlp_t: false # use mlp on L instead of transformer
puzzle_emb_len: 0 # no puzzle embeddings for text
no_ACT_continue: true # No continue ACT loss, only use the sigmoid of the halt which makes much more sense

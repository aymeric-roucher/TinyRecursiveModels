# Text training config

defaults:
  - arch: trm_text
  - _self_

hydra:
  output_subdir: null

# Data path
data_paths: ['data/text']
data_paths_test: []

evaluators: []

# Hyperparams - Training
global_batch_size: 32
gradient_accumulation_steps: 4

epochs: 5  # Needed to match nanoGPT's 5k steps with batch_size=64
eval_interval: 1
min_eval_interval: 1
checkpoint_every_eval: true

lr: 5e-4
lr_min_ratio: 0.5
lr_warmup_steps: 100  # Longer warmup with larger batch size

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.0

# Hyperparams - Puzzle embeddings training (not used for text)
puzzle_emb_lr: 0.0

seed: 0

ema: false
ema_rate: 0.999
freeze_weights: false

# Benchmarking
benchmark_mode: false
benchmark_steps: null
